#**Behavioral Cloning** 

# **[Behavioral Cloning](https://github.com/rakshithkeegadi/CarND-Behavioral-Cloning-P3/blob/master/model.py)** 
by [Rakshith Krishnamurthy](https://www.linkedin.com/in/rakshith-krishnamurthy-360682b/)

---

**Behavioral Cloning Project**

The goals / steps of this project are the following:
* Use the simulator to collect data of good driving behavior
* Build, a convolution neural network in Keras that predicts steering angles from images
* Train and validate the model with a training and validation set
* Test that the model successfully drives around track one without leaving the road
* Summarize the results with a written report


### Files Submitted & Code Quality

#### 1. Submission includes all required files and can be used to run the simulator in autonomous mode

My project includes the following files:
* model.py containing the script to create and train the model
* drive.py for driving the car in autonomous mode
* model.h5 containing a trained convolution neural network 
* car_cam_track1.mp4 video of the car central camera view for track1
* car_cam_track2.mp4 video of the car central camera view for track2

#### 2. Submission includes functional code
Using the Udacity provided simulator and my drive.py file, the car can be driven autonomously around the track by executing 
```sh
python drive.py model.h5
```

#### 3. Submission code is usable and readable

The model.py file contains the code for training and saving the convolution neural network. The file shows the pipeline I used for training and validating the model, and it contains comments to explain how the code works.

### Model Architecture and Training Strategy

#### 1. An appropriate model architecture has been employed

The following Nvidia's convolutional neural architecture was used to train the model. 
The architecture below implementation can be seen in model.py. (Lines 79- 93)

![Model Architecture](https://github.com/rakshithkeegadi/CarND-Behavioral-Cloning-P3/blob/master/examples/cnn-architecture-624x890.png) 

#### 2. Attempts to reduce overfitting in the model

To avoid overfitting I had to use a lesser number of epochs as low as 5 because the validation loss kept fluctulating for higher epochs. (Line 73 model.py)

#### 3. Model parameter tuning

The model used an adam optimizer, so the learning rate was set to 0.0001. (Line 73 model.py)

#### 4. Appropriate training data

Training data was generated by driving my car as much as possible on the middle of both the tracks. The data was collected by driving the car on both track 1 and track 2.
3 laps of data was collected for track1 whereas 2 laps of data was collected for track2.

![Data Visualization](https://github.com/rakshithkeegadi/CarND-Behavioral-Cloning-P3/blob/master/examples/Data%20Visulaization.png) 

The graph above shows the steering angles of the car at different parts of the tracks collected both for track1 and track2. 
The value ranges from -1 to +1 defining the steering values for left and right. Steering value of 0 means there was no steering required.


### Model Architecture and Training Strategy

#### 1. Solution Design Approach

I first started off collecting large samples of data just for track1 and then used lenet architecture. Unfortunately, the car was not driving smooth enough on the middle of the track.

I decided to used [this](https://devblogs.nvidia.com/parallelforall/deep-learning-self-driving-cars/) article to derive the Nvidia's architecture where the car was driving much smoother than just using Lenet architecture. 

I tried using the recovery methods to center the car but that was not successful but still there were curves where the car would drift off and take the other path.

#### Left Cam
![Left Img](https://github.com/rakshithkeegadi/CarND-Behavioral-Cloning-P3/blob/master/examples/left_steering_0.jpg) 
#### Center Cam
![Center Img](https://github.com/rakshithkeegadi/CarND-Behavioral-Cloning-P3/blob/master/examples/center_steering_0.jpg)
#### Right Cam
![Right Img](https://github.com/rakshithkeegadi/CarND-Behavioral-Cloning-P3/blob/master/examples/right_steering_0.jpg)

Then I just tried driving my car in the opposite direction to collect more data. This time the car performed much better but still I could see it drift off the road and cut the lanes.  

#### Cropped Image
![Cropped Img](https://github.com/rakshithkeegadi/CarND-Behavioral-Cloning-P3/blob/master/examples/cropped.jpg)

I also tried cropping the images so that the model just focuses on the road rather than the scenary around. Now, I saw improvement but still there was more scope because the problem of cutting the lanes was still not resloved for track1. 

#### 2. Final Model Architecture

After spending hours and hours driving the track and collecting data I was very disappointed with how my car was performing.
This time I wanted to try something different beacuse collecting such large data took me a lot of time for training the model.
The constant uploading data to aws and training took a lot of time.

To reduce the large data I first started using just the training data by driving around the center of the track as much as I can and then flip the images

![regularImg](https://github.com/rakshithkeegadi/CarND-Behavioral-Cloning-P3/blob/master/examples/readImg.jpg) ![flippedImg](https://github.com/rakshithkeegadi/CarND-Behavioral-Cloning-P3/blob/master/examples/flipped.jpg)

This reduced a lot of data and training became easier and the steering was multiplied by -1.0.(model.py Line 56)
I also added a steering correction factor for the left and right images. The steering correction factor of 0.25 worked the best and this maintained the steering of the car without oversteering it.

Still I could see sometimes in track1 the car would drift off and cut the border lanes. drive.py sends RGB images to the model, but cv2.imread() reads images in BGR format so I converted the image from BGR to RGB and then trained the model.

This gave me the result I was looking for track1 and the car drove smoother than ever in the middle of the road.
Click the links for track1 [car cam  video](https://github.com/rakshithkeegadi/CarND-Behavioral-Cloning-P3/blob/master/car_cam_track1.mp4) and [aerial view.](https://www.youtube.com/watch?v=igtkdac6__Q&feature=youtu.be) 

I used the same model with just track1 data to run on track2. The car did a good job of driving in the middle till it hit steep curves. This problem was dealt with by gathering data for track2 by driving the car in the center of the road as much as I can and capturing steering angles for steep curves.

Here are the images for neg -1.0 steering angle images for track2

![left neg](https://github.com/rakshithkeegadi/CarND-Behavioral-Cloning-P3/blob/master/examples/left_neg_1.jpg) ![center neg](https://github.com/rakshithkeegadi/CarND-Behavioral-Cloning-P3/blob/master/examples/center_neg_1.jpg) ![right_neg](https://github.com/rakshithkeegadi/CarND-Behavioral-Cloning-P3/blob/master/examples/right_neg_1.jpg)

After using the data from track2 I could drive my car in track2 without the going over the edge and hitting barricades the video for track2. Click the links for track2 [car cam video](https://github.com/rakshithkeegadi/CarND-Behavioral-Cloning-P3/blob/master/car_cam_track2.mp4) and [aerial view.](https://www.youtube.com/watch?v=yImksKPnCds&feature=youtu.be)
